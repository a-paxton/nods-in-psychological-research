---
title: 'Introduction to APIs'
author: "A. Paxton"
output:
  html_document:
    keep_md: yes
    number_sections: yes
---

An API---or *application programming interface*---is a formal avenue created 
by an organization or website to programmatically access their data. We could
most directly contrast an API with *web scraping*, which we will cover in a 
future session.

To use an API, you must submit a *request* to the system. That request
includes a set of information that has been specified by the API developers.
Minimally, the request will describe the kind of data that you want to access
(in the manner outlined by the API). For some systems, this must also include an
*API key*, which is an authenticated permission identifier that allows you to
access data. 

After submitting your request, you will receive a *response* that contains the
data you've requested. Your next job will be to parse the response into usable
data.

In this exercise, we'll walk through the process of using an API directly from R.

***

# Preliminaries

First, we'll need to prepare for our exercises. We'll do this by loading the
packages we need, including---if needed---installing those packages. For the
sake of the exercise, we'll load the packages silently, meaning that we won't
clutter our R markdown output with a bunch of warnings and output messages
from the loading and installation process.

```{r clear-workspace}

# clear the workspace (useful if we're not knitting)
rm(list=ls())

```

```{r function-check-for-packages, include=FALSE}

# make sure we can load packages 
# (thanks to https://gist.github.com/smithdanielle/9913897)
load_or_install_packages <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, 
                     dependencies = TRUE,
                     repos="http://cloud.r-project.org/")
  sapply(pkg, require, character.only = TRUE)
}

```

```{r load-packages, message=FALSE, results="hide"}

# specify which packages we'll need
required_packages = c("tidyverse",
                      "httr",
                      "jsonlite")

# install them (if necessary) and load them
load_or_install_packages(required_packages)

```

***

# Finding APIs

Not every website, repository, or organization has an API available for users to
access data, but those that do have them tend to make them easily available. You
might start by checking the website for an API information link. This might be
at the bottom of the webpage or available in a "Developers" menu of the website.
If you can't find one, try using the "find" function in your browser for `API`,
or try searching `API` in the website's search function (if they have one).

If you still can't find it, try doing a general search on your favorite search
engine with the name of the website and API. (As you may already know, putting
both terms in separate double-quotes will ensure you only get webpage results
that include both terms.)

If you still can't find anything, it may be time to turn to web scraping---but,
again, we won't be covering that here. (Again, keep in mind matters of
ethicality and legality, including the *hiQ Labs v. LinkedIn* ruling in the US.)

***

# Submitting a request

For this exercise, we'll be accessing the [NYC Open Data
repository](https://opendata.cityofnewyork.us/). This rich dataset includes open
data from a variety of government sources from New York, NY (USA). If you take
time to poke around the website, you'll see a large number of datasets
available. Let's choose the [2015 Street Tree Census
data](https://data.cityofnewyork.us/Environment/2015-Street-Tree-Census-Tree-Data/pi5s-9p35).

For this, we'll be using the `httr` library's `GET` function.

## Identify API URL

To get started, we need to figure out more about this API. A good place to start
is to take a look at the website. Once you get to the platform, you should see
(as of February 2021) an interactive map of the tree census with a few boxes of
words above the upper-right corner of the map. If you click "Export," a new
sidebar will appear, and you'll get access to a number of options (yes,
including the option to just download a CSV of the data). One option will be the
"SODA API" (or *Socrata Open Data API (SODA)*), which we'll use for this exercise. 

Once you click on the SODA API link, you'll see new information pop up,
including lots of helpful links to additional documentation about the SODA API
and a list of fields in this dataset. After looking at the API information, we 
know the URL of our dataset. Let's create a variable to hold onto that.

```{r specify-api-url}

# specify the API location
tree_data_location = "https://data.cityofnewyork.us/resource/5rq2-4hqu.json"

```

## Identify filtering parameters for request

Next, we decide what subset of the data we'd like to receive.
Unfortunately, the metadata isn't terribly detailed, so let's click on a few
trees on the map to find out what some examples might be. 

This might feel clunky or inelegant, but keep in mind that a lot of working with
naturally occurring data requires persistence and creativity---along with all of
the usual admonitions to know your data!

After poking around, let's say that we want to find all trees in the Bronx. In
looking at the metadata list and some of the options in the visualized dataset,
we've found the variable name (`zip_city`) and value (`Bronx`) for it. We need
to prepare the variable in a `list` item, which `httr::GET()` expects.

When we input the values, we need to respect the data type in R and in the API.
Given that `Bronx` is a string, we'll need to remember to encapsulate it in
double-quotes so that R doesn't get angry with us.

```{r specify-api-filtering-parameters}

# specify what we want to filter by
return_variables = list(zip_city = "Bronx")

```

